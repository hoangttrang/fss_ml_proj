{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e46b0778-bf65-4dfd-bcc6-6e3c4b0941c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MLP Hyperparameter Tuning and Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "896a9da6-3c9e-4d40-b9af-cdd564d074c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, NumericType, DateType, DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.sql.functions import col, count, mean, stddev, min, max, when, isnan, countDistinct, lit, to_timestamp, to_date, hour, avg, sum, date_sub, current_date, datediff, floor, row_number, date_format, weekofyear, year, round, substring, concat, regexp_replace, lag, last, rank, unix_timestamp, expr, ntile, rand, monotonically_increasing_id\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import optuna\n",
    "import gc\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7500cfdb-ec36-43b1-b733-302f501bbee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "app_name = \"basic-training\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24a391f8-0efe-4f81-bf58-127ed4e48091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183d7ddd-18fc-4d6a-9a0d-25905b5e8f75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data = spark.read.parquet(\"dbfs:/FileStore/tables/train_data3.parquet\")\n",
    "test_data = spark.read.parquet(\"dbfs:/FileStore/tables/test_data3.parquet\")\n",
    "\n",
    "\n",
    "# switch is_accident to label\n",
    "train_data = train_data.withColumn(\"label\", (col(\"is_accident\") == 1).cast(IntegerType())).drop(\"is_accident\")\n",
    "test_data = test_data.withColumn(\"label\", (col(\"is_accident\") == 1).cast(IntegerType())).drop(\"is_accident\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d279508d-94a4-4797-9d53-02c9e665b3b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop labelm id columns , and EDA feature \n",
    "drop_columns = ['zipcode',\"trip_date\",  'vehicle_id_encoded', 'driver_id_encoded']\n",
    "\n",
    "# drop \n",
    "drop_columns.extend([\"year\", \"month\"])\n",
    "\n",
    "train_data = train_data.drop(*drop_columns)\n",
    "test_data = test_data.drop(*drop_columns)\n",
    "\n",
    "schema = train_data.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b6a1d1b-a75d-4df9-95dc-119c554a904f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set up pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "597f7a54-2783-4339-888e-fc2e43c904c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numerical columns: []\n"
     ]
    }
   ],
   "source": [
    "# find non-numerical columns\n",
    "numerical_cols = [field.name for field in schema if isinstance(field.dataType, NumericType) and field.name != \"label\"]\n",
    "non_numerical_cols = [field.name for field in schema if not isinstance(field.dataType, NumericType)]\n",
    "\n",
    "# Show results\n",
    "print(\"Non-numerical columns:\", non_numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c18d2cd5-540d-4fd1-8d9f-d320a26d8d88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive (label=1): 260\nNegative (label=0): 166819\nScale positive weight: 641.6115384615384\n"
     ]
    }
   ],
   "source": [
    "label_counts=train_data.groupBy('label').count()\n",
    "counts = {row[\"label\"]: row[\"count\"] for row in label_counts.collect()}\n",
    "\n",
    "# Assign positive and negative counts\n",
    "num_positive = counts.get(1, 0)\n",
    "num_negative = counts.get(0, 0)\n",
    "\n",
    "print(f\"Positive (label=1): {num_positive}\")\n",
    "print(f\"Negative (label=0): {num_negative}\")\n",
    "\n",
    "# create POS weight\n",
    "scale_pos_weight = num_negative / num_positive\n",
    "print(f\"Scale positive weight: {scale_pos_weight}\")\n",
    "\n",
    "train_data = train_data.withColumn(\n",
    "    \"class_weight\",\n",
    "    when(col(\"label\") == 1, scale_pos_weight).otherwise(1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db91a5d4-91a1-4557-a36d-41dcf0811221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Assemble Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb941eb3-a8d1-46bf-b092-6863289a4af3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop zero importance feature columns from GBTClassifier (and all group columns)\n",
    "zero_importance_features = ['last_has_issues',\n",
    " 'vehicle_rolling_15trip_num_issues',\n",
    " 'vehicle_rolling_15trip_issues_distance',\n",
    " 'vehicle_rolling_15trip_issues_minutes',\n",
    " 'vehicle_rolling_15trip_issues_events',\n",
    " 'vehicle_rolling_15trip_issues_given_date',\n",
    " 'prev_sum_travel_distance',\n",
    " 'prev_sum_minutes_driving',\n",
    " 'years_since_min_trip_date',\n",
    " 'driver_sum_travel_distance_roll15d',\n",
    " 'driver_sum_minutes_driving_roll15d',\n",
    " 'crash_roll15d',\n",
    " 'driver_facing_cam_obstruction_roll15d',\n",
    " 'drowsiness_roll15d',\n",
    " 'forward_collision_warning_roll15d',\n",
    " 'manual_event_roll15d',\n",
    " 'near_miss_roll15d',\n",
    " 'ran_a_red_light_roll15d',\n",
    " 'road_facing_cam_obstruction_roll15d',\n",
    " 'stop_sign_violation_roll15d',\n",
    " 'unsafe_lane_change_roll15d',\n",
    " 'total_events_per_trip_roll15d',\n",
    " 'high_roll15d',\n",
    " 'ratio_camera_obstruction_roll15d_per_year',\n",
    " 'ratio_crash_roll15d_per_year',\n",
    " 'ratio_driver_facing_cam_obstruction_roll15d_per_year',\n",
    " 'ratio_drowsiness_roll15d_per_year',\n",
    " 'ratio_forward_collision_warning_roll15d_per_year',\n",
    " 'ratio_manual_event_roll15d_per_year',\n",
    " 'ratio_ran_a_red_light_roll15d_per_year',\n",
    " 'ratio_road_facing_cam_obstruction_roll15d_per_year',\n",
    " 'ratio_seat_belt_violation_roll15d_per_year',\n",
    " 'ratio_tailgating_roll15d_per_year',\n",
    " 'ratio_unsafe_lane_change_roll15d_per_year',\n",
    " 'ratio_total_events_per_trip_roll15d_per_year',\n",
    " 'ratio_trip_ts_roll15d_per_year',\n",
    " 'ratio_low_roll15d_per_year',\n",
    " 'ratio_high_roll15d_per_year',\n",
    " 'prev_review_rate_per_km',\n",
    " 'prev_review_rate_per_min',\n",
    " 'prev_review_rate_per_event',\n",
    " 'group_wi__buckys',\n",
    " 'group_ga_psi_augusta',\n",
    " 'group_nc__denver',\n",
    " 'group_tn__memphis__safety_quip',\n",
    " 'group_fl__freedom',\n",
    " 'group_fl__jw_craft',\n",
    " 'group_ms__american_johnny',\n",
    " 'group_fl__gainsville_porta_serve',\n",
    " 'group_pa__port_a_bowl',\n",
    " 'group_ia__cedar_rapids',\n",
    " 'group_ga__gci',\n",
    " 'group_ms__gotta_go',\n",
    " 'group_pa__malvern',\n",
    " 'group_tx__forza',\n",
    " 'group_tn__fusionsite_(clark)',\n",
    " 'group_san_angelo_tops_septic',\n",
    " 'group_wi__stranders',\n",
    " 'group_az__stamback_admin',\n",
    " 'group_oh__rent__a__john',\n",
    " 'group_az__stamback_rolloffs_',\n",
    " 'group_nc__griffin_hook_trucks',\n",
    " 'group_tn__maxwell_septic',\n",
    " 'group_ky__lex',\n",
    " 'group_nc__asc',\n",
    " 'group_tn__chattanooga_bolles',\n",
    " 'group_tn__etp_',\n",
    " 'group_sc__littlejohn',\n",
    " 'group_tx__acp',\n",
    " 'group_va__r_r',\n",
    " 'group_sc__psi_columbia',\n",
    " 'group_ky__bullitt_sep_service',\n",
    " 'group_ky__moon_minis',\n",
    " 'group_ky__moon_leasing',\n",
    " 'group_nc__greensboro',\n",
    " 'group_tn__fusionsite_(woodycrest)',\n",
    " 'group_ky__moon_portables',\n",
    " 'group_tx__j_bar',\n",
    " 'group_ar__fay',\n",
    " 'group_lubbock_hd_',\n",
    " 'group_az__stamback_septic',\n",
    " 'group_kermit',\n",
    " 'group_lubbock',\n",
    " 'group_wi__cesspool',\n",
    " 'group_tn__fusionsite_(nashville)',\n",
    " 'group_tn__mc_septic',\n",
    " 'group_ny__a__john',\n",
    " 'group_nc__griffin_waste_pump_trucks',\n",
    " 'group_oh__c_l_and_safeway',\n",
    " 'group_ar__little_rock'\n",
    " 'total_fatalities_prev_3m_avg',\n",
    " 'total_vehicles_prev_3m_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "057f4f1d-01c1-4ed5-a691-a6c7b6f35537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop zero_importance_features from train_df and test_df\n",
    "train_data = train_data.drop(*zero_importance_features)\n",
    "test_data = test_data.drop(*zero_importance_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7933e46e-c5f4-43c9-a2ba-6a510a499655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to make sure train/val split is stratified \n",
    "def stratified_split(df, label_col=\"label\", train_frac=0.8, seed=42):\n",
    "    fractions = df.select(label_col).distinct().withColumn(\n",
    "        \"fraction\", F.lit(train_frac)\n",
    "    ).rdd.collectAsMap()\n",
    "    \n",
    "    # Sample by class\n",
    "    train_subset = df.stat.sampleBy(label_col, fractions, seed)\n",
    "    \n",
    "    # Get the validation set as the remaining data\n",
    "    # Using a left anti join to get rows in df that aren't in train_subset\n",
    "    validation_subset = df.join(\n",
    "        train_subset, \n",
    "        on=df.columns, \n",
    "        how=\"left_anti\"\n",
    "    )\n",
    "    return train_subset, validation_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae3560b5-e39e-49d3-9901-1cd3f7c3a03a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746c24b5-4ece-4eda-83f8-da1b272969a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, TrainValidationSplit\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d9ede8-e1c9-43a0-aed9-4c68678a4543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_subset, validation_subset = stratified_split(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0492349a-5de0-41f1-98e4-209a99bb15c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Combine the train and validation subsets with a column indicating which is which\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Add a column to indicate training vs validation data\n",
    "train_with_indicator = train_subset.withColumn(\"train_val_indicator\", lit(0))  # 0 for training\n",
    "val_with_indicator = validation_subset.withColumn(\"train_val_indicator\", lit(1))  # 1 for validation\n",
    "\n",
    "# Union the datasets\n",
    "combined_data = train_with_indicator.union(val_with_indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0b7eacd-b4a8-4f3a-9ff1-c9736ab82b4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_with_indicator = train_subset.withColumn(\"train_val_indicator\", lit(0))  # 0 for training\n",
    "val_with_indicator = validation_subset.withColumn(\"train_val_indicator\", lit(1))  # 1 for validation\n",
    "\n",
    "# Union the datasets\n",
    "combined_data = train_with_indicator.union(val_with_indicator)\n",
    "\n",
    "feature_cols = [col for col in train_data.columns if col not in ['label', 'class_weight', 'train_val_indicator']]\n",
    "\n",
    "# Assemble features into a single vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"assembled_features\")\n",
    "\n",
    "# Add StandardScaler to normalize the features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"assembled_features\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Get the number of features and classes\n",
    "num_features = len(feature_cols)\n",
    "num_classes = train_data.select(\"label\").distinct().count()\n",
    "\n",
    "# Define the MLP stage without the weightCol parameter if it's causing issues\n",
    "mlp_no_early_stopping = MultilayerPerceptronClassifier(\n",
    "    labelCol=\"label\", \n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"prediction\",\n",
    "    layers=[num_features, 10, int(num_classes)],  # Initial value\n",
    "    maxIter=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create a pipeline with all stages\n",
    "pipeline_no_early_stopping = Pipeline(stages=[assembler, scaler, mlp_no_early_stopping])\n",
    "\n",
    "# Create a focused parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(mlp_no_early_stopping.layers, [\n",
    "        [num_features, 10, int(num_classes)], \n",
    "        [num_features, 16, 8, int(num_classes)]\n",
    "    ]) \\\n",
    "    .addGrid(mlp_no_early_stopping.stepSize, [0.001, 0.01, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "# Create the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Create the CrossValidator with the train_val_indicator column\n",
    "custom_validator = CrossValidator(\n",
    "    estimator=pipeline_no_early_stopping,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=2,\n",
    "    foldCol=\"train_val_indicator\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc1f6bd7-1cc3-4d6e-8f71-b96bc6723e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43065541842475e9681c3fbcd543e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c56225e5a3c43c496dc700bb9242fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual feature size: 52\n"
     ]
    }
   ],
   "source": [
    "# Making sure dimensions make sense before running full scale tuning \n",
    "test_pipeline = Pipeline(stages=[assembler, scaler])\n",
    "transformed_data = test_pipeline.fit(combined_data).transform(combined_data)\n",
    "actual_feature_size = len(transformed_data.select(\"features\").first()[0])\n",
    "print(f\"Actual feature size: {actual_feature_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "799b2090-6919-480b-8024-4c36695082ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual feature size: 52\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actual feature size: {actual_feature_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5e1957-2907-42a5-a902-04b7037aba6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "custom_model = custom_validator.fit(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e416ff-1df6-43a1-beb0-2b8be1988a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_pipeline_model = custom_model.bestModel\n",
    "best_mlp_model = best_pipeline_model.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "853ff3e4-a367-4bec-b729-ebfbd1ad9c62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.5031010844704371,\n",
       " 0.5031010844704371,\n",
       " 0.5031010844704371,\n",
       " 0.5065232321126227,\n",
       " 0.5065232321126227,\n",
       " 0.5065232321126227]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340cf50f-6b51-49a7-98f9-3aa6e814d406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd076db-ffbc-470c-b02e-a6ae2151b0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters:\nLayers: [52, 16, 8, 2]\nStep Size: 0.001\nTest AUROC: 0.49972322476363396\n"
     ]
    }
   ],
   "source": [
    "# Print best model parameters\n",
    "print(\"Best model parameters:\")\n",
    "print(f\"Layers: {best_mlp_model.getLayers()}\")\n",
    "print(f\"Step Size: {best_mlp_model.getStepSize()}\")\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = best_pipeline_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "auroc = evaluator.evaluate(predictions)\n",
    "print(f\"Test AUROC: {auroc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c22c5c3b-973d-4f22-9706-a90e427de764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_model = custom_model.bestModel\n",
    "\n",
    "# Now train the final model on all data\n",
    "# First, combine all your data without the train/val split\n",
    "final_data = train_data  # Assuming this is your complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca99ae3-fadb-4f7a-bd33-a6dac4236e22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the final pipeline with the same components as before\n",
    "final_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"assembled_features\")\n",
    "final_scaler = StandardScaler(\n",
    "    inputCol=\"assembled_features\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# Extract the best parameters from the CV model\n",
    "best_layers = best_model.stages[-1].getLayers()  # Extract best layer configuration\n",
    "best_step_size = best_model.stages[-1].getStepSize()  # Extract best step size\n",
    "\n",
    "# Define the final MLP model with the best parameters\n",
    "final_mlp = MultilayerPerceptronClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"prediction\",\n",
    "    layers=best_layers,\n",
    "    stepSize=best_step_size,\n",
    "    maxIter=100,  \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create the final pipeline\n",
    "final_pipeline = Pipeline(stages=[final_assembler, final_scaler, final_mlp])\n",
    "\n",
    "# Train the final model on all data\n",
    "final_model = final_pipeline.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78a142b6-48cf-4b44-ba1a-7a45443b4dd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUROC: 0.5040901229374094\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data\n",
    "final_test_predictions = final_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "auroc = evaluator.evaluate(final_test_predictions)\n",
    "print(f\"Test AUROC: {auroc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc12a7e1-64d5-4637-a8b0-81ff9925b78c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate metrics using PySpark DataFrame operations\n",
    "from pyspark.sql.functions import sum, when, col\n",
    "\n",
    "# Calculate TP, FP, TN, FN using DataFrame operations\n",
    "metrics = final_test_predictions.select(\n",
    "    sum(when(col(\"prediction\") == 1.0, 1).otherwise(0)).alias(\"predicted_positives\"),\n",
    "    sum(when(col(\"label\") == 1.0, 1).otherwise(0)).alias(\"actual_positives\"),\n",
    "    sum(when((col(\"prediction\") == 1.0) & (col(\"label\") == 1.0), 1).otherwise(0)).alias(\"true_positives\"),\n",
    "    sum(when((col(\"prediction\") == 1.0) & (col(\"label\") == 0.0), 1).otherwise(0)).alias(\"false_positives\"),\n",
    "    sum(when((col(\"prediction\") == 0.0) & (col(\"label\") == 0.0), 1).otherwise(0)).alias(\"true_negatives\"),\n",
    "    sum(when((col(\"prediction\") == 0.0) & (col(\"label\") == 1.0), 1).otherwise(0)).alias(\"false_negatives\")\n",
    ").collect()[0]\n",
    "\n",
    "# Extract values from the Row\n",
    "predicted_positives = metrics.predicted_positives\n",
    "actual_positives = metrics.actual_positives\n",
    "actual_negatives = final_test_predictions.count() - actual_positives\n",
    "true_positives = metrics.true_positives\n",
    "false_positives = metrics.false_positives\n",
    "true_negatives = metrics.true_negatives\n",
    "false_negatives = metrics.false_negatives\n",
    "\n",
    "# Calculate TPR, FPR, and PPV\n",
    "tpr = float(true_positives) / float(actual_positives) if actual_positives > 0 else 0.0\n",
    "fpr = float(false_positives) / float(actual_negatives) if actual_negatives > 0 else 0.0\n",
    "ppv = float(true_positives) / float(predicted_positives) if predicted_positives > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a97fa7e7-e4b4-4c27-8573-375553301614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR (False Positive Rate): 0.0009\nPPV (Positive Predictive Value): 0.000000\n"
     ]
    }
   ],
   "source": [
    "print(f\"FPR (False Positive Rate): {fpr:.4f}\")\n",
    "print(f\"PPV (Positive Predictive Value): {ppv:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9860c042-46a6-4b0c-bd3f-a0c1b31a9184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(true_positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d13d1e93-63cf-465d-95d2-914129472a59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The model did not predict any accidents. This is likely because we did not use a weighted loss function or have any way to address the class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77c13e9c-9e26-4abc-9c5f-d37b94dc6ba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mlp_hypertuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}